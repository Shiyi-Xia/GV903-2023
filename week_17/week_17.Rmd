---
title: "Generliased Linear Model: Bootstrapping"
subtitle: "Week 17"
author: "Winnie Xia"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
library(bootstrap)
library(ggplot2)
library(tidyverse)
library(fontawesome)
```
class: inverse, center, middle

# Get Started

What is the difference between parametric or non-parametric?

---
# Bootstrap

- **Definition**: It usually refers to a self-starting process that is to proceed withot external input.
- Applied to statistics: We sample with replace from the sample
---
# Bootstrap
Bootstrap is a desirable approach when:
- **the distribution of a statistic is unknown or complicated**.
- **Reason**: bootstrap is a non-parametric and does not ask foor specific distributions.
- **the sample size is too small to draw a valid inference.**
- **Reason**: it is a resampling method with replacement and recreates any number of resamples.
---
# Let's breal down "bootstrap"

Bootstrap breaks down into the following steps:
- decide how many bootstrap samples to perform.
- what is the sample size?
- for each bootstrap sample:
 - draw a sample with replacement with the chosen size
 - calculate the statistic of interest for that sample 
- calculate the mean of the calculated sample statistics.

---
# Bootstrapping Illustration in R

```{r tidy=FALSE}
set.seed(123)
n <- 20
s <- rnorm(n = n,
           mean = 5.5,
           sd = 1.4)
bp_s <- bootstrap::bootstrap(s, 1000, var)
str(bp_s)
```

---
# Or we could write the bootstrap() manually

.pull-left[
```{r plot-last, fig.show = 'hide'}
B <- matrix(0, nrow = 1000, 
            ncol = n)

for (i in 1: 1000){
  B[i, ] <- sample(s, size = n,
                   replace = TRUE)
}

sd_b <- apply(B, 1, sd)
hist(sd_b)
```
]
.pull-right[
```{r ref.label = 'plot-last', echo = FALSE}
```
]
---
class: center, middle 

Other Sampling Approaches 
---
# Jackknife
**It is a leave-one-out procedure.**
It means:
- We copy the existing sample *n* times, and each time, we delete one but different observation.
- Then, we calculate the statistics of interests. 

---
# Jackknife Illustration in R

---
# *K*-cross Validation
- **Divide the data into k parts and predict one left out segment based on a model of the remaining k âˆ’ 1 segments;**
- **Then assess distribution of prediction error.**

![](kfold.png)

---
# Permutation Tests
- To compare outcomes in experiments, we often do a two-sample t-test. 
- It assumes that data are randomly selected from the population, arrived in large samples (>30), or normally distributed with equal variances between groups.
- But we could also o a permutation test,**without any distributional assumptions.**

---
# Permutation Tests in R
